{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Given code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_int(f):\n",
    "    ba = bytearray(4)\n",
    "    f.readinto(ba)\n",
    "    prm = np.frombuffer(ba, dtype=np.int32)\n",
    "    return prm[0]\n",
    "\n",
    "def read_double(f):\n",
    "    ba = bytearray(8)\n",
    "    f.readinto(ba)\n",
    "    prm = np.frombuffer(ba, dtype=np.double)\n",
    "    return prm[0]\n",
    "\n",
    "def read_double_tab(f, n):\n",
    "    ba = bytearray(8*n)\n",
    "    nr = f.readinto(ba)\n",
    "    if nr != len(ba):\n",
    "        return []\n",
    "    else:\n",
    "        prm = np.frombuffer(ba, dtype=np.double)\n",
    "        return prm\n",
    "\n",
    "def get_pics_from_file(filename):\n",
    "    f_pic = open(filename, \"rb\")\n",
    "    info = dict()\n",
    "    info[\"nb_pics\"] = read_int(f_pic)\n",
    "    info[\"freq_sampling_khz\"] = read_double(f_pic)\n",
    "    info[\"freq_trame_hz\"] = read_double(f_pic)\n",
    "    info[\"freq_pic_khz\"] = read_double(f_pic)\n",
    "    info[\"norm_fact\"] = read_double(f_pic)\n",
    "    tab_pics = []\n",
    "    pics = read_double_tab(f_pic, info[\"nb_pics\"])\n",
    "    nb_trames = 1\n",
    "    while len(pics) > 0:\n",
    "        nb_trames = nb_trames + 1\n",
    "        tab_pics.append(pics)\n",
    "        pics = read_double_tab(f_pic, info[\"nb_pics\"])\n",
    "    f_pic.close()\n",
    "    return tab_pics, info"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Our Algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### To visualize binaries as PNGs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# for file in glob.glob(\"../data/pics_*.bin\"):\n",
    "#    key = file.split(\"pics_\")[1].replace(\".bin\", \"\")\n",
    "#    values, info = get_pics_from_file(file)\n",
    "#    plt.imshow(values,aspect=\"auto\",)\n",
    "#    plt.savefig(\"../data/PNGs/pics_\" + key + \".png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Libraries used"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix,f1_score,accuracy_score\n",
    "import glob\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creation of training and testing sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "\n",
    "X,Y = [],[]\n",
    "\n",
    "# Average value of NOKEY\n",
    "pics_nokey, info = get_pics_from_file(\"../data/pics_NOKEY.bin\")\n",
    "nokey = np.mean(pics_nokey,axis=0)\n",
    "\n",
    "for file in glob.glob(\"../data/pics_*.bin\"):\n",
    "    key = file.split(\"pics_\")[1].replace(\".bin\",\"\")\n",
    "    if key == \"LOGINMDP\":\n",
    "        continue\n",
    "    values, info = get_pics_from_file(file)\n",
    "\n",
    "    # Sampling the key signal\n",
    "    np.random.shuffle(values)\n",
    "    values = values[:3000]\n",
    "\n",
    "    # Reducing noise\n",
    "    values = np.subtract(values,nokey)\n",
    "\n",
    "    for frame in values:\n",
    "        X.append(frame)\n",
    "        Y.append(key)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Data scaling\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KNN Model\n",
    "<font color=\"red\"> A ne pas Ã©xecuter lors du test </font>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nclassifier = KNeighborsClassifier(n_neighbors=40, p=2, metric=\\'euclidean\\',n_jobs=-1)\\nclassifier.fit(X_train,Y_train)\\nY_pred = classifier.predict(X_test)\\n\\n#### K-cross validation ####\\n# from sklearn.model_selection import cross_val_score\\n# cv_scores = cross_val_score(classifier,X,Y, cv=10)\\n# print(cv_scores)\\n# print(np.mean(cv_scores))\\n\\n### Evaluating model ###\\nprint(\"Accuracy score:\", accuracy_score(Y_test,Y_pred),\"%\")\\n\\n# from sklearn.metrics import classification_report\\n# print(classification_report(Y_test, Y_pred))\\n\\n# cm = confusion_matrix(Y_test,Y_pred)\\n# print(cm)\\n'"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "classifier = KNeighborsClassifier(n_neighbors=40, p=2, metric='euclidean',n_jobs=-1)\n",
    "classifier.fit(X_train,Y_train)\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "#### K-cross validation ####\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# cv_scores = cross_val_score(classifier,X,Y, cv=10)\n",
    "# print(cv_scores)\n",
    "# print(np.mean(cv_scores))\n",
    "\n",
    "### Evaluating model ###\n",
    "print(\"Accuracy score:\", accuracy_score(Y_test,Y_pred),\"%\")\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "# cm = confusion_matrix(Y_test,Y_pred)\n",
    "# print(cm)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### To save the current model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# pickle.dump(classifier,open(\"./KNN_model.sav\",'wb'))\n",
    "# pickle.dump(sc_X,open(\"./scaling_function.sav\",'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing noise and segmenting signal\n",
    "\n",
    "<font color=\"#D5B60A\"> Some chunks are missing so can't be used in the project </font>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ndef pics_distance(lline, rline):\\n    return np.linalg.norm(lline - rline)\\n\\nguess, info = get_pics_from_file(\"../data/pics_LOGINMDP.bin\")\\n\\nguess = np.subtract(guess, nokey)\\nchunks = [] # contains the lines of each chunk\\nchunks_index = [] # location of the start of each chunk\\nempty = np.subtract(nokey, nokey)\\ni = 0\\nthreshold = 1.4 # threshold to detect a nokey\\nmin_nokey_length = 10 # consider there is a nokey based on this length\\nwhile i < len(guess):\\n    nokey_length = 0\\n    while i < len(guess) and pics_distance(guess[i], empty) < threshold:\\n        nokey_length += 1\\n        i += 1\\n    if nokey_length > min_nokey_length:\\n        chunks.append([])\\n        chunks_index.append(i)\\n    while i < len(guess) and pics_distance(guess[i], empty) >= threshold:\\n        if len(chunks) == 0:\\n            chunks.append([])\\n            chunks_index.append(i)\\n        chunks[-1].append(guess[i])\\n        i += 1\\n\\n# cleaning noise\\nj = 0\\nwhile j < len(chunks):\\n    if len(chunks[j]) < 5:\\n        chunks.pop(j)\\n        chunks_index.pop(j)\\n    else:\\n        j += 1\\n\\nprint(len(chunks_index))\\nprint(chunks_index)\\n# for chunk in chunks:\\n#     print(len(chunk))\\n'"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def pics_distance(lline, rline):\n",
    "    return np.linalg.norm(lline - rline)\n",
    "\n",
    "guess, info = get_pics_from_file(\"../data/pics_LOGINMDP.bin\")\n",
    "\n",
    "guess = np.subtract(guess, nokey)\n",
    "chunks = [] # contains the lines of each chunk\n",
    "chunks_index = [] # location of the start of each chunk\n",
    "empty = np.subtract(nokey, nokey)\n",
    "i = 0\n",
    "threshold = 1.4 # threshold to detect a nokey\n",
    "min_nokey_length = 10 # consider there is a nokey based on this length\n",
    "while i < len(guess):\n",
    "    nokey_length = 0\n",
    "    while i < len(guess) and pics_distance(guess[i], empty) < threshold:\n",
    "        nokey_length += 1\n",
    "        i += 1\n",
    "    if nokey_length > min_nokey_length:\n",
    "        chunks.append([])\n",
    "        chunks_index.append(i)\n",
    "    while i < len(guess) and pics_distance(guess[i], empty) >= threshold:\n",
    "        if len(chunks) == 0:\n",
    "            chunks.append([])\n",
    "            chunks_index.append(i)\n",
    "        chunks[-1].append(guess[i])\n",
    "        i += 1\n",
    "\n",
    "# cleaning noise\n",
    "j = 0\n",
    "while j < len(chunks):\n",
    "    if len(chunks[j]) < 5:\n",
    "        chunks.pop(j)\n",
    "        chunks_index.pop(j)\n",
    "    else:\n",
    "        j += 1\n",
    "\n",
    "print(len(chunks_index))\n",
    "print(chunks_index)\n",
    "# for chunk in chunks:\n",
    "#     print(len(chunk))\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running the model on a signal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "classifier = pickle.load(open(\"./KNN_model.sav\",'rb'))\n",
    "sc_X = pickle.load(open(\"./scaling_function.sav\",'rb'))\n",
    "\n",
    "# Loading the signal\n",
    "input, info = get_pics_from_file(\"../data/pics_LOGINMDP.bin\")\n",
    "\n",
    "# Average value of NOKEY\n",
    "pics_nokey, info = get_pics_from_file(\"../data/pics_NOKEY.bin\")\n",
    "nokey = np.mean(pics_nokey,axis=0)\n",
    "\n",
    "output = []\n",
    "step = 90\n",
    "\n",
    "for i in range(30000000):\n",
    "    if (step * (i + 1)) >= len(input):\n",
    "        break\n",
    "    curr = input[step * i:step * (i + 1)] # chunking\n",
    "    curr = np.subtract(curr, nokey) # removing noise\n",
    "    curr = sc_X.transform(curr) # scaling\n",
    "    curr_output = classifier.predict(curr)\n",
    "\n",
    "    # Picking best guess\n",
    "    unique, pos = np.unique(curr_output, return_inverse=True)\n",
    "    max_value = unique[np.bincount(pos).argmax()]\n",
    "\n",
    "    if max_value == 'NOKEY':\n",
    "        continue\n",
    "\n",
    "    # Guessing key pressed with the SHIFT key (if there is)\n",
    "    if max_value == 'SHIFT':\n",
    "        curr = input[step * i:step * (i + 1)]\n",
    "\n",
    "        # Removing the meaningful value of SHIFT's signal\n",
    "        for j, frame in enumerate(curr):\n",
    "            frame[5] = 0\n",
    "            curr[j] = frame\n",
    "\n",
    "        curr = np.subtract(curr,nokey)\n",
    "        curr = sc_X.transform(curr)\n",
    "        curr_output = classifier.predict(curr)\n",
    "        unique, pos = np.unique(curr_output, return_inverse=True)\n",
    "        max_value = unique[np.bincount(pos).argmax()]\n",
    "\n",
    "        if max_value == 'NOKEY' or max_value == 'SHIFT':\n",
    "            continue # Nothing added to input\n",
    "        if output == [] or (\"SHIFT \" + max_value) != output[-1]:\n",
    "            output.append(\"SHIFT \" + max_value)\n",
    "\n",
    "    elif output == [] or max_value != output[-1]:\n",
    "        output.append(max_value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input's length is: 20\n",
      "CTRL\n",
      "SUPPR\n",
      "SHIFT C\n",
      "SHIFT G\n",
      "SHIFT W\n",
      "SHIFT P\n",
      "SHIFT I\n",
      "H\n",
      "A\n",
      "C\n",
      "K\n",
      "A\n",
      "G\n",
      "O\n",
      "N\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "ENTER\n"
     ]
    }
   ],
   "source": [
    "print(\"The input's length is:\", len(output))\n",
    "for a in output:\n",
    "    print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}